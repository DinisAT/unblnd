{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brainstorming Diferent Module Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 01:19:21.426634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-17 01:19:22.286292: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-17 01:19:24.156628: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 01:19:24.157005: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-17 01:19:24.157021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import EncoderDecoderModel, EncoderDecoderConfig, BertConfig\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, BertForMaskedLM, BertTokenizer\n",
    "from transformers import T5Tokenizer\n",
    "import torch.optim as optim\n",
    "import concurrent.futures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Art', 'Science', 'Technology']\n",
    "keywords = ['Painting', 'Physics', 'Computer Science']\n",
    "questions = ['What is you preferred hobbie?', 'What is a favorite discipline?',\n",
    "             'Is python your programming language?']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "# cs, key, q = zip(*batch)\n",
    "#     top_enc = tokenizer(topics, padding=True, truncation=True, return_tensors='pt')\n",
    "#     key_enc = tokenizer(keywords, padding=True, truncation=True, return_tensors='pt')\n",
    "#     q_enc = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "# print(top_enc)\n",
    "# print(key_enc)\n",
    "# print(q_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5-base? t5-small?\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_data = []\n",
    "# for t, k in zip(topics, keywords):\n",
    "#     prompt = f\"Topic: {t} | Keyword: {k} | Question: \"\n",
    "#     t_data.append(prompt)\n",
    "# print(t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i = []\n",
    "# t = []\n",
    "# for t, k, q in zip(topics, keywords, questions):\n",
    "#     input_str = f\"generate a question for topic {topic} and keyword {keyword}:\"\n",
    "#     i_tokens = tokenizer.tokenize(i_str)\n",
    "#     i_ids = tokenizer.convert_tokens_to_ids(i_tokens)\n",
    "#     i_ids = torch.tensor(i_ids)\n",
    "#     t_str = f\"{q} [SEP]\"\n",
    "#     t_tokens = tokenizer.tokenize(t_str)\n",
    "#     t_ids = tokenizer.convert_tokens_to_ids(t_tokens)\n",
    "#     t_ids = torch.tensor(t_ids)\n",
    "#     i.append(i_ids)\n",
    "#     t.append(t_ids)\n",
    "# inputs = torch.nn.utils.rnn.pad_sequence(i, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "# targets = torch.nn.utils.rnn.pad_sequence(t, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "# attention_mask = inputs != tokenizer.pad_token_id\n",
    "# print(inputs) \n",
    "# print(attention_mask) \n",
    "# print(targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Options "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerations to build my model:\n",
    "\n",
    "1. set of topic-keyword pairs \n",
    "2. prepare a set of training prompts that include the topic, keyword, and a placeholder for the question. \n",
    "3. fine-tune the GPT-2 model on the training prompts by minimizing the difference between the model's predicted next-token and the actual next-token. \n",
    "4. generate questions for each topic-keyword pair by providing the trained model with a prompt that includes the topic, keyword, and the question placeholder, and using the generate method to generate a new question based on the provided prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch and Hugging Face Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence-to-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class TopicKeywordDataset(Dataset):\n",
    "    def __init__(self, topics, keywords, questions):\n",
    "        self.topics = topics\n",
    "        self.keywords = keywords\n",
    "        self.questions = questions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.topics)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        topic = self.topics[idx]\n",
    "        keyword = self.keywords[idx]\n",
    "        question = self.questions[idx]\n",
    "        return (topic, keyword, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loadr(batch):\n",
    "    topics, keywords, questions = zip(*batch)\n",
    "    topics_enc = tokenizer(topics, padding=True, truncation=True, return_tensors='pt')\n",
    "    keywords_enc = tokenizer(keywords, padding=True, truncation=True, return_tensors='pt')\n",
    "    questions_enc = tokenizer(questions, padding=True, truncation=True, return_tensors='pt')\n",
    "    return topics_enc, keywords_enc, questions_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TopicKeywordDataset(topics, keywords, questions)\n",
    "data_loader = DataLoader(dataset, batch_size=3, collate_fn=data_loadr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model configuration and encoder-decoder model\n",
    "encoder_config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, encoder_config)\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased', config=decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# print(model)\n",
    "# break\n",
    "model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch in data_loader:\n",
    "        topics_enc, keywords_enc, questions_enc = batch\n",
    "        input_ids = torch.cat([topics_enc['input_ids'], keywords_enc['input_ids']], dim=1)\n",
    "        # print(input_ids)\n",
    "        output_ids = questions_enc['input_ids']\n",
    "        attention_mask = torch.cat([topics_enc['attention_mask'], keywords_enc['attention_mask']], dim=1)\n",
    "        # print(attention_mask)\n",
    "        decoder_attention_mask = questions_enc['attention_mask']\n",
    "        # print(decoder_attention_mask)\n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=output_ids, decoder_attention_mask=decoder_attention_mask).loss\n",
    "        # loss.backward() #forward did not return a valid loss value\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating a question generator?\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# generated_questions = []\n",
    "# for topic, keyword in zip(topics, keywords):\n",
    "#     input_text = f\"generate question for {topic} and {keyword}\"\n",
    "#     input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors='pt')\n",
    "#     # input_ids = tokenizer.encode(f\"generate question for {topic} and {keyword}\", return_tensors='pt')\n",
    "#     generated = model.generate(input_ids, max_length=50, do_sample=True, \n",
    "#                                decoder_start_token_id=tokenizer.encode(tokenizer.bos_token)[0])\n",
    "#     question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "#     generated_questions.append(question)\n",
    "\n",
    "# generated_questions = []\n",
    "# for topic, keyword in zip(topics, keywords):\n",
    "#     input_ids = tokenizer.encode(f\"generate question for {topic} and {keyword}\", return_tensors='pt')\n",
    "#     generated = model.generate(\n",
    "#         input_ids, \n",
    "#         max_length=50, \n",
    "#         do_sample=True,\n",
    "#         generation_config=EncoderDecoderGenerationConfig(bos_token_id=tokenizer.bos_token_id)\n",
    "#     )\n",
    "#     question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "#     generated_questions.append(question)\n",
    "\n",
    "generated_questions = []\n",
    "for topic, keyword in zip(topics, keywords):\n",
    "    input_text = f\"generate question for {topic} and {keyword}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    # generated_ids = model.generate(input_ids, max_length=50, do_sample=True, decoder_start_token_id=tokenizer.bos_token_id)\n",
    "    generated = model.generate(input_ids, max_length=50, do_sample=True, \n",
    "                           decoder_start_token_id=model.config.decoder.pad_token_id)\n",
    "\n",
    "    generated_question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    generated_questions.append(generated_question)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['. body weres and and and and because because because are because because because because because as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as as', '. body body and utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility utility', '. representative representative representative representative representative representative of of silver silver silver silver bronze bronze silver silver silver bronze silver silver silver silver bronze bronze bronze silver silver silver silver gold silver silver silver silver silver silver silver silver silver silver silver silver silver silver silver silver silver silver']\n"
     ]
    }
   ],
   "source": [
    "print(generated_questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for topic, keyword in zip(topics, keywords):\n",
    "    prompt = f\"Topic: {topic} | Keyword: {keyword} | Question: \"\n",
    "    train_data.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for prompt in train_data:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        output = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# evaluating a question generator?\n",
    "model.eval()\n",
    "generated_questions = []\n",
    "for topic, keyword in zip(topics, keywords):\n",
    "    prompt = f\"Topic: {topic} | Keyword: {keyword} | Question: \"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "    question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    generated_questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Topic: Art | Keyword: Painting | Question: __________________\\n\\nI hope that your art is well presented. Let us know if you can use some.', \"Topic: Science | Keyword: Physics | Question: 【How does a planet in the solar system work】 | Question: (what's the physics aspect of a particular planet) | Question: (where is the planet, the planet's orbit\", 'Topic: Technology | Keyword: Computer Science | Question: -------------- next part -------------- An HTML attachment was scrubbed... URL: <http://mail.reuters.com/id/pdp-sydna.html>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_questions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Field' from 'torchtext.data' (/home/dinis/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/torchtext/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Field\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/home/dinis/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/torchtext/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopicKeywordDataset(Dataset):\n",
    "    def __init__(self, topics, keywords, questions):\n",
    "        self.topics = topics\n",
    "        self.keywords = keywords\n",
    "        self.questions = questions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.topics)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        topic = self.topics[idx]\n",
    "        keyword = self.keywords[idx]\n",
    "        question = self.questions[idx]\n",
    "        return (topic, keyword, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loadr_2(batch):\n",
    "    topics, keywords, questions = zip(*batch)\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    max_input_len = max(len(tokenizer.tokenize(f\"generate a question for topic {topic} and keyword {keyword}:\")) + len(tokenizer.tokenize(question)) + 1 for topic, keyword, question in batch) # plus 1 for [SEP] token\n",
    "    \n",
    "    # for topic, keyword, question in zip(topics, keywords, questions):\n",
    "    for topic, keyword, question in batch:\n",
    "        input_str = f\"generate a question for topic {topic} and keyword {keyword}: {question} [SEP]\"\n",
    "        input_tokens = tokenizer.tokenize(input_str)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        inputs.append(input_ids)\n",
    "        # print(\"Input shape:\", input_ids.size())  \n",
    "        target_tokens = input_tokens[1:-1]  # exclude [CLS] and [SEP] tokens\n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        # target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_ids += [-100] * (max_input_len - len(target_ids))  # pad with -100\n",
    "        target_ids = torch.tensor(target_ids)\n",
    "        targets.append(target_ids)\n",
    "        # print(\"Target shape:\", target_ids.size())\n",
    "        \n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # pad the inputs\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    # targets_flat = [t for target in targets for t in target]\n",
    "#     targets_batch = targets_flat[:batch_size]\n",
    "    # pad the targets\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)\n",
    "    # targets = torch.nn.functional.pad(targets, (0, max_target_len - targets.shape[1]), value=tokenizer.pad_token_id)\n",
    "    \n",
    "    \n",
    "    # for i, (input_seq, target_seq) in enumerate(zip(inputs, targets)):\n",
    "    #     print(f\"Batch {i}: input shape={input_seq.shape}, target shape={target_seq.shape}\")  \n",
    "    \n",
    "    attention_mask = inputs != tokenizer.pad_token_id\n",
    "    return inputs, attention_mask, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loadr_2(batch):\n",
    "#     inputs = [torch.tensor(item[0], dtype=torch.long) for item in batch]\n",
    "#     targets = [torch.tensor(item[1], dtype=torch.float) for item in batch]\n",
    "    \n",
    "#     # Print statements to see how the inputs and targets are being processed\n",
    "#     print(\"inputs before padding:\", inputs)\n",
    "#     print(\"targets before padding:\", targets)\n",
    "    \n",
    "#     inputs = pad_sequence(inputs, batch_first=True)\n",
    "#     targets = pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "#     # Print statements to see how the inputs and targets look after padding\n",
    "#     print(\"inputs after padding:\", inputs)\n",
    "#     print(\"targets after padding:\", targets)\n",
    "    \n",
    "#     return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    " \n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = questions[0]\n",
    "topic = topics[0]\n",
    "bert_tokenized_sequence = tokenizer.tokenize(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length BERT:  10\n"
     ]
    }
   ],
   "source": [
    "max_len_bert = 0\n",
    "max_len_roberta = 0\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    # `[CLS]`  `[SEP]` tokens.\n",
    "    input_ids_bert = tokenizer.encode(question, add_special_tokens=True)\n",
    "\n",
    "    \n",
    "    max_len_bert = max(max_len_bert, len(input_ids_bert))\n",
    "\n",
    "    \n",
    "print('Max sentence length BERT: ', max_len_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "topic_counts = Counter(topics)\n",
    "topic_to_label = {topic: i for i, (topic, _) in enumerate(topic_counts.most_common())}\n",
    "\n",
    "labels = torch.tensor([topic_to_label[topic] for topic in topics])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TopicKeywordDataset(topics, keywords, questions)\n",
    "data_loader_2 = DataLoader(dataset, batch_size=3, collate_fn=data_loadr_2, drop_last=True)\n",
    "for i, batch in enumerate(data_loader_2):\n",
    "    print(f\"Batch {i}: {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Is python your programming language?\n",
      "Token IDs BERT: tensor([2054, 2003, 1037, 5440, 9009, 1029,    0,    0,    0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinis/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_input_ids = []\n",
    "bert_attention_masks = []\n",
    "question_ids = []\n",
    "counter = 0\n",
    "\n",
    "for question in questions:\n",
    "    bert_encoded_dict = tokenizer.encode_plus(\n",
    "                        question,                      # Sentence to encode.\n",
    "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 10,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt', )# Return pytorch tensors.\n",
    "    bert_input_ids.append(bert_encoded_dict['input_ids'])\n",
    "    bert_attention_masks.append(bert_encoded_dict['attention_mask'])\n",
    "    \n",
    "    \n",
    "    question_ids.append(counter)\n",
    "    counter  = counter + 1\n",
    "    \n",
    "bert_input_ids = torch.cat(bert_input_ids, dim=0)\n",
    "bert_attention_masks = torch.cat(bert_attention_masks, dim=0)\n",
    "\n",
    "# labels = torch.tensor(topics)\n",
    "question_ids = torch.tensor(question_ids)\n",
    "\n",
    "print('Original: ', question)\n",
    "print('Token IDs BERT:', bert_input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m data_loader_2:\n\u001b[1;32m      5\u001b[0m         inputs, attention_mask, targets \u001b[39m=\u001b[39m batch\n\u001b[1;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader_2' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for batch in data_loader_2:\n",
    "        inputs, attention_mask, targets = batch\n",
    "        print(inputs)\n",
    "        print(targets)\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# model.train()\n",
    "# for epoch in range(5):\n",
    "#     for batch in data_loader_2:\n",
    "#         try:\n",
    "#             inputs, attention_mask, targets = batch\n",
    "#             outputs = model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#         except Exception as e:\n",
    "#             print(\"Error occurred:\", e)\n",
    "#             print(\"Inputs:\", inputs)\n",
    "#             print(\"Targets:\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# question generation\n",
    "model.eval()\n",
    "generated_questions = []\n",
    "for topic, keyword in zip(topics, keywords):\n",
    "    input_str = f\"generate a question for topic {topic} and keyword {keyword}:\"\n",
    "    input_tokens = tokenizer.tokenize(input_str)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    generated = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "    question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    generated_questions.append(question)\n",
    "\n",
    "print(generated_questions)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paralleling the models with a final estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with a RandomForest Final Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input_data = ...\n",
    "\n",
    "# def preprocess_data(input_data):\n",
    "#     preprocessed_data = ...\n",
    "#     return preprocessed_data\n",
    "\n",
    "# def run_gpt2_model(input_data):\n",
    "#     gpt2_results = ...\n",
    "#     return gpt2_results\n",
    "\n",
    "# def run_pytorch_model(input_data):\n",
    "#     pytorch_results = ...\n",
    "#     return pytorch_results\n",
    "\n",
    "# def run_bert_model(input_data):\n",
    "#     bert_results = ...\n",
    "#     return bert_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_estimator = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, &lt;function preprocess_data at 0x7f10589d4a60&gt;),\n",
       "                (&#x27;modeling&#x27;,\n",
       "                 &lt;concurrent.futures.thread.ThreadPoolExecutor object at 0x7f10507e2770&gt;),\n",
       "                (&#x27;gpt2_model&#x27;, &lt;function run_gpt2_model at 0x7f1050b8bf40&gt;),\n",
       "                (&#x27;pytorch_model&#x27;,\n",
       "                 &lt;function run_pytorch_model at 0x7f1050bcc040&gt;),\n",
       "                (&#x27;bert_model&#x27;, &lt;function run_bert_model at 0x7f1050bcc0d0&gt;),\n",
       "                (&#x27;final_estimator&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, &lt;function preprocess_data at 0x7f10589d4a60&gt;),\n",
       "                (&#x27;modeling&#x27;,\n",
       "                 &lt;concurrent.futures.thread.ThreadPoolExecutor object at 0x7f10507e2770&gt;),\n",
       "                (&#x27;gpt2_model&#x27;, &lt;function run_gpt2_model at 0x7f1050b8bf40&gt;),\n",
       "                (&#x27;pytorch_model&#x27;,\n",
       "                 &lt;function run_pytorch_model at 0x7f1050bcc040&gt;),\n",
       "                (&#x27;bert_model&#x27;, &lt;function run_bert_model at 0x7f1050bcc0d0&gt;),\n",
       "                (&#x27;final_estimator&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function preprocess_data at 0x7f10589d4a60&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ThreadPoolExecutor</label><div class=\"sk-toggleable__content\"><pre>&lt;concurrent.futures.thread.ThreadPoolExecutor object at 0x7f10507e2770&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function run_gpt2_model at 0x7f1050b8bf40&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function run_pytorch_model at 0x7f1050bcc040&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function run_bert_model at 0x7f1050bcc0d0&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', <function preprocess_data at 0x7f10589d4a60>),\n",
       "                ('modeling',\n",
       "                 <concurrent.futures.thread.ThreadPoolExecutor object at 0x7f10507e2770>),\n",
       "                ('gpt2_model', <function run_gpt2_model at 0x7f1050b8bf40>),\n",
       "                ('pytorch_model',\n",
       "                 <function run_pytorch_model at 0x7f1050bcc040>),\n",
       "                ('bert_model', <function run_bert_model at 0x7f1050bcc0d0>),\n",
       "                ('final_estimator', RandomForestClassifier())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('preprocessing', preprocess_data),\n",
    "#     ('modeling', concurrent.futures.ThreadPoolExecutor(max_workers=3)),\n",
    "#     ('gpt2_model', run_gpt2_model),\n",
    "#     ('pytorch_model', run_pytorch_model),\n",
    "#     ('bert_model', run_bert_model),\n",
    "#     ('final_estimator', final_estimator)\n",
    "# ])\n",
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(topics, keywords):\n",
    "    train_data = []\n",
    "    for topic, keyword in zip(topics, keywords):\n",
    "        prompt = f\"Topic: {topic} | Keyword: {keyword} | Question: \"\n",
    "        train_data.append(prompt)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(train_data):\n",
    "    model.train()\n",
    "    for epoch in range(5):\n",
    "        for prompt in train_data:\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "            output = model(input_ids=input_ids, labels=input_ids)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    generated_questions = []\n",
    "    for topic, keyword in zip(topics, keywords):\n",
    "        prompt = f\"Topic: {topic} | Keyword: {keyword} | Question: \"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        generated = model.generate(input_ids, max_length=50, do_sample=True)\n",
    "        question = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        generated_questions.append(question)\n",
    "    return generated_questions\n",
    "\n",
    "\n",
    "gpt_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocess_data),\n",
    "    ('question_generation', generate_questions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, &lt;function preprocess_data at 0x7f104f2bc700&gt;),\n",
       "                (&#x27;question_generation&#x27;,\n",
       "                 &lt;function generate_questions at 0x7f104f2bc4c0&gt;)])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessing&#x27;, &lt;function preprocess_data at 0x7f104f2bc700&gt;),\n",
       "                (&#x27;question_generation&#x27;,\n",
       "                 &lt;function generate_questions at 0x7f104f2bc4c0&gt;)])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function preprocess_data at 0x7f104f2bc700&gt;</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">function</label><div class=\"sk-toggleable__content\"><pre>&lt;function generate_questions at 0x7f104f2bc4c0&gt;</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessing', <function preprocess_data at 0x7f104f2bc700>),\n",
       "                ('question_generation',\n",
       "                 <function generate_questions at 0x7f104f2bc4c0>)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requesting open AI to start a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13ffb9c9d7c12dd8966884fbc25fad89fa777e4b21056cada8ddbb2886f41748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
